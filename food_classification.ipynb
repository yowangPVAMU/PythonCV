{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4668475,"sourceType":"datasetVersion","datasetId":2708274}],"dockerImageVersionId":30301,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-08T05:42:56.449509Z","iopub.execute_input":"2022-12-08T05:42:56.450019Z","iopub.status.idle":"2022-12-08T05:42:56.640429Z","shell.execute_reply.started":"2022-12-08T05:42:56.449936Z","shell.execute_reply":"2022-12-08T05:42:56.639506Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n# Note: this notebook requires torch >= 1.10.0\ntorch.__version__","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:56.64221Z","iopub.execute_input":"2022-12-08T05:42:56.642777Z","iopub.status.idle":"2022-12-08T05:42:58.721932Z","shell.execute_reply.started":"2022-12-08T05:42:56.642741Z","shell.execute_reply":"2022-12-08T05:42:58.720809Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:58.726149Z","iopub.execute_input":"2022-12-08T05:42:58.727427Z","iopub.status.idle":"2022-12-08T05:42:58.733968Z","shell.execute_reply.started":"2022-12-08T05:42:58.727382Z","shell.execute_reply":"2022-12-08T05:42:58.73309Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"/kaggle/input/practice-food101/\")\nimage_path = data_path / \"Food-101-Practice-data(three class)\"\nimage_path","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:58.736088Z","iopub.execute_input":"2022-12-08T05:42:58.736591Z","iopub.status.idle":"2022-12-08T05:42:58.750153Z","shell.execute_reply.started":"2022-12-08T05:42:58.73656Z","shell.execute_reply":"2022-12-08T05:42:58.749099Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\ndef walk_through_dir(dir_path):\n  \"\"\"\n  Walks through dir_path returning its contents.\n  Args:\n    dir_path (str or pathlib.Path): target directory\n  \n  Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n  \"\"\"\n  for dirpath, dirnames, filenames in os.walk(dir_path):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:58.753957Z","iopub.execute_input":"2022-12-08T05:42:58.755163Z","iopub.status.idle":"2022-12-08T05:42:58.761079Z","shell.execute_reply.started":"2022-12-08T05:42:58.755112Z","shell.execute_reply":"2022-12-08T05:42:58.760121Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"walk_through_dir(image_path)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:58.762167Z","iopub.execute_input":"2022-12-08T05:42:58.762795Z","iopub.status.idle":"2022-12-08T05:42:58.77879Z","shell.execute_reply.started":"2022-12-08T05:42:58.762756Z","shell.execute_reply":"2022-12-08T05:42:58.777385Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:58.780264Z","iopub.execute_input":"2022-12-08T05:42:58.780713Z","iopub.status.idle":"2022-12-08T05:42:58.785955Z","shell.execute_reply.started":"2022-12-08T05:42:58.780675Z","shell.execute_reply":"2022-12-08T05:42:58.785312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom PIL import Image\n\n# Set seed\n#random.seed(42) # <- try changing this and see what happens\n\n# 1. Get all image paths (* means \"any combination\")\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# 2. Get random image path\nrandom_image_path = random.choice(image_path_list)\n\n# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\nimage_class = random_image_path.parent.stem\n\n# 4. Open image\nimg = Image.open(random_image_path)\n\n# 5. Print metadata\nprint(f\"Random image path: {random_image_path}\")\nprint(f\"Image class: {image_class}\")\nprint(f\"Image height: {img.height}\") \nprint(f\"Image width: {img.width}\")\nimg","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:58.786997Z","iopub.execute_input":"2022-12-08T05:42:58.787799Z","iopub.status.idle":"2022-12-08T05:42:58.908497Z","shell.execute_reply.started":"2022-12-08T05:42:58.787775Z","shell.execute_reply":"2022-12-08T05:42:58.907687Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Turn the image into an array\nimg_as_array = np.asarray(img)\n\n# Plot the image with matplotlib\nplt.figure(figsize=(10, 7))\nplt.imshow(img_as_array)\nplt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels]\")\nplt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:58.909395Z","iopub.execute_input":"2022-12-08T05:42:58.909691Z","iopub.status.idle":"2022-12-08T05:42:59.124478Z","shell.execute_reply.started":"2022-12-08T05:42:58.909664Z","shell.execute_reply":"2022-12-08T05:42:59.123747Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:59.127589Z","iopub.execute_input":"2022-12-08T05:42:59.128014Z","iopub.status.idle":"2022-12-08T05:42:59.432433Z","shell.execute_reply.started":"2022-12-08T05:42:59.127986Z","shell.execute_reply":"2022-12-08T05:42:59.431396Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Write transform for image\ndata_transform = transforms.Compose([\n    # Resize the images to 64x64\n    transforms.Resize(size=(64, 64)),\n    # Flip the images randomly on the horizontal\n    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n])","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:59.434901Z","iopub.execute_input":"2022-12-08T05:42:59.435319Z","iopub.status.idle":"2022-12-08T05:42:59.44084Z","shell.execute_reply.started":"2022-12-08T05:42:59.435281Z","shell.execute_reply":"2022-12-08T05:42:59.439375Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1.->Testing Transformation with onne image","metadata":{}},{"cell_type":"code","source":"test_img_path =random.sample(image_path_list, k=1)\npil_image = Image.open(str(test_img_path[0]))\ntest_transformed_image = data_transform(pil_image).permute(1, 2, 0) \nplt.figure(figsize=(10, 7))\nplt.imshow(test_transformed_image)\nplt.title(f\"Transformed \\nSize: {test_transformed_image.shape}\")\nplt.axis(False)\nplt.suptitle(f\"Class: {test_img_path[0].parent.stem}\", fontsize=16)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:59.442595Z","iopub.execute_input":"2022-12-08T05:42:59.443508Z","iopub.status.idle":"2022-12-08T05:42:59.571878Z","shell.execute_reply.started":"2022-12-08T05:42:59.443473Z","shell.execute_reply":"2022-12-08T05:42:59.57077Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_transformed_images(image_paths, transform, n=3, seed=42):\n    \"\"\"Plots a series of random images from image_paths.\n\n    Will open n image paths from image_paths, transform them\n    with transform and plot them side by side.\n\n    Args:\n        image_paths (list): List of target image paths. \n        transform (PyTorch Transforms): Transforms to apply to images.\n        n (int, optional): Number of images to plot. Defaults to 3.\n        seed (int, optional): Random seed for the random generator. Defaults to 42.\n    \"\"\"\n    random.seed(seed)\n    random_image_paths = random.sample(image_paths, k=n)\n    #print(f'random_image_paths : {random_image_paths}')\n    for image_path in random_image_paths:\n        #print(f'type of image_path :{type(image_path)}')\n        with Image.open(image_path) as f:\n            fig, ax = plt.subplots(1, 2)\n            ax[0].imshow(f) \n            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n            ax[0].axis(\"off\")\n\n            # Transform and plot image\n            # Note: permute() will change shape of image to suit matplotlib \n            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n            transformed_image = transform(f).permute(1, 2, 0) \n            ax[1].imshow(transformed_image) \n            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n            ax[1].axis(\"off\")\n\n            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n\nplot_transformed_images(image_path_list, \n                        transform=data_transform, \n                        n=10)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:42:59.57548Z","iopub.execute_input":"2022-12-08T05:42:59.576205Z","iopub.status.idle":"2022-12-08T05:43:01.702734Z","shell.execute_reply.started":"2022-12-08T05:42:59.576171Z","shell.execute_reply":"2022-12-08T05:43:01.702088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Image data into torchvision.datasets.ImageFolder.","metadata":{}},{"cell_type":"code","source":"# Use ImageFolder to create dataset(s)\nfrom torchvision import datasets\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.70376Z","iopub.execute_input":"2022-12-08T05:43:01.704563Z","iopub.status.idle":"2022-12-08T05:43:01.718548Z","shell.execute_reply.started":"2022-12-08T05:43:01.704535Z","shell.execute_reply":"2022-12-08T05:43:01.716771Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get class names as a list\nclass_names = train_data.classes\nclass_names","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.720459Z","iopub.execute_input":"2022-12-08T05:43:01.720953Z","iopub.status.idle":"2022-12-08T05:43:01.731483Z","shell.execute_reply.started":"2022-12-08T05:43:01.720915Z","shell.execute_reply":"2022-12-08T05:43:01.729967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.733018Z","iopub.execute_input":"2022-12-08T05:43:01.733527Z","iopub.status.idle":"2022-12-08T05:43:01.742617Z","shell.execute_reply.started":"2022-12-08T05:43:01.733494Z","shell.execute_reply":"2022-12-08T05:43:01.740768Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the lengths\nlen(train_data), len(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.744029Z","iopub.execute_input":"2022-12-08T05:43:01.744369Z","iopub.status.idle":"2022-12-08T05:43:01.755356Z","shell.execute_reply.started":"2022-12-08T05:43:01.744342Z","shell.execute_reply":"2022-12-08T05:43:01.754215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, label = train_data[0][0], train_data[0][1]\nprint(f\"Image tensor:\\n{img}\")\nprint(f\"Image shape: {img.shape}\")\nprint(f\"Image datatype: {img.dtype}\")\nprint(f\"Image label: {label}\")\nprint(f\"Label datatype: {type(label)}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.756661Z","iopub.execute_input":"2022-12-08T05:43:01.757021Z","iopub.status.idle":"2022-12-08T05:43:01.792488Z","shell.execute_reply.started":"2022-12-08T05:43:01.756987Z","shell.execute_reply":"2022-12-08T05:43:01.791481Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rearrange the order of dimensions\nimg_permute = img.permute(1, 2, 0)\n\n# Print out different shapes (before and after permute)\nprint(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\nprint(f\"Image permute shape: {img_permute.shape} -> [height, width, color_channels]\")\n\n# Plot the image\nplt.figure(figsize=(10, 7))\nplt.imshow(img.permute(1, 2, 0))\nplt.axis(\"off\")\nplt.title(class_names[label], fontsize=14);","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.79396Z","iopub.execute_input":"2022-12-08T05:43:01.794336Z","iopub.status.idle":"2022-12-08T05:43:01.885826Z","shell.execute_reply.started":"2022-12-08T05:43:01.794299Z","shell.execute_reply":"2022-12-08T05:43:01.88484Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data # this is mainly pythorch data set","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.887831Z","iopub.execute_input":"2022-12-08T05:43:01.888155Z","iopub.status.idle":"2022-12-08T05:43:01.895876Z","shell.execute_reply.started":"2022-12-08T05:43:01.888128Z","shell.execute_reply":"2022-12-08T05:43:01.895063Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.897307Z","iopub.execute_input":"2022-12-08T05:43:01.897613Z","iopub.status.idle":"2022-12-08T05:43:01.908118Z","shell.execute_reply.started":"2022-12-08T05:43:01.89758Z","shell.execute_reply":"2022-12-08T05:43:01.907328Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.909514Z","iopub.execute_input":"2022-12-08T05:43:01.910074Z","iopub.status.idle":"2022-12-08T05:43:01.987247Z","shell.execute_reply.started":"2022-12-08T05:43:01.910021Z","shell.execute_reply":"2022-12-08T05:43:01.986109Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import Tuple, Dict, List\ndef find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n    \"\"\"Finds the class folder names in a target directory.\n    \n    Assumes target directory is in standard image classification format.\n\n    Args:\n        directory (str): target directory to load classnames from.\n\n    Returns:\n        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n    \n    Example:\n        find_classes(\"food_images/train\")\n        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n    \"\"\"\n    # 1. Get the class names by scanning the target directory\n    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n    \n    # 2. Raise an error if class names not found\n    if not classes:\n        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n        \n    # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return classes, class_to_idx","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:01.988657Z","iopub.execute_input":"2022-12-08T05:43:01.98903Z","iopub.status.idle":"2022-12-08T05:43:01.996664Z","shell.execute_reply.started":"2022-12-08T05:43:01.988989Z","shell.execute_reply":"2022-12-08T05:43:01.995778Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TinyVGG(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*16*16,\n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        # print(x.shape)\n        x = self.conv_block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:43:35.945032Z","iopub.execute_input":"2022-12-08T05:43:35.945401Z","iopub.status.idle":"2022-12-08T05:43:35.966367Z","shell.execute_reply.started":"2022-12-08T05:43:35.945372Z","shell.execute_reply":"2022-12-08T05:43:35.965101Z"},"trusted":true},"outputs":[],"execution_count":null}]}